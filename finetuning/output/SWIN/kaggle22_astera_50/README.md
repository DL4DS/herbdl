---
license: apache-2.0
base_model: microsoft/swinv2-tiny-patch4-window8-256
tags:
- image-classification
- vision
- generated_from_trainer
metrics:
- accuracy
model-index:
- name: kaggle22_astera_50
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# kaggle22_astera_50

This model is a fine-tuned version of [microsoft/swinv2-tiny-patch4-window8-256](https://huggingface.co/microsoft/swinv2-tiny-patch4-window8-256) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 3.7057
- Accuracy: 0.0688

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.005
- train_batch_size: 128
- eval_batch_size: 128
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 100.0

### Training results

| Training Loss | Epoch | Step  | Accuracy | Validation Loss |
|:-------------:|:-----:|:-----:|:--------:|:---------------:|
| No log        | 1.0   | 216   | 0.0688   | 3.7220          |
| No log        | 2.0   | 432   | 0.0688   | 3.7102          |
| 3.7271        | 3.0   | 648   | 0.0688   | 3.7086          |
| 3.7271        | 4.0   | 864   | 0.0688   | 3.7089          |
| 3.7146        | 5.0   | 1080  | 0.0688   | 3.7088          |
| 3.7146        | 6.0   | 1296  | 0.0688   | 3.7097          |
| 3.7145        | 7.0   | 1512  | 0.0688   | 3.7082          |
| 3.7145        | 8.0   | 1728  | 0.0688   | 3.7073          |
| 3.7145        | 9.0   | 1944  | 0.0688   | 3.7068          |
| 3.7158        | 10.0  | 2160  | 0.0688   | 3.7118          |
| 3.7158        | 11.0  | 2376  | 0.0688   | 3.7084          |
| 3.7165        | 12.0  | 2592  | 0.0688   | 3.7075          |
| 3.7165        | 13.0  | 2808  | 0.0688   | 3.7074          |
| 3.7107        | 14.0  | 3024  | 0.0688   | 3.7067          |
| 3.7107        | 15.0  | 3240  | 0.0688   | 3.7059          |
| 3.7107        | 16.0  | 3456  | 0.0688   | 3.7060          |
| 3.7104        | 17.0  | 3672  | 0.0688   | 3.7060          |
| 3.7104        | 18.0  | 3888  | 0.0688   | 3.7057          |
| 3.7104        | 19.0  | 4104  | 0.0688   | 3.7058          |
| 3.7104        | 20.0  | 4320  | 0.0688   | 3.7056          |
| 3.7219        | 11.0  | 4741  | 0.0688   | 3.7174          |
| 3.7188        | 12.0  | 5172  | 0.0688   | 3.7170          |
| 3.7203        | 13.0  | 5603  | 0.0688   | 3.7091          |
| 3.7135        | 14.0  | 6034  | 0.0688   | 3.7086          |
| 3.7135        | 15.0  | 6465  | 0.0688   | 3.7070          |
| 3.714         | 16.0  | 6896  | 0.0688   | 3.7095          |
| 3.7134        | 17.0  | 7327  | 0.0688   | 3.7124          |
| 3.7121        | 18.0  | 7758  | 0.0688   | 3.7085          |
| 3.7164        | 19.0  | 8189  | 0.0688   | 3.7076          |
| 3.7133        | 20.0  | 8620  | 0.0688   | 3.7084          |
| 3.7119        | 21.0  | 9051  | 0.0688   | 3.7075          |
| 3.7119        | 22.0  | 9482  | 0.0688   | 3.7076          |
| 3.7125        | 23.0  | 9913  | 0.0688   | 3.7083          |
| 3.7128        | 24.0  | 10344 | 0.0688   | 3.7092          |
| 3.7105        | 25.0  | 10775 | 0.0688   | 3.7066          |
| 3.7132        | 26.0  | 11206 | 0.0688   | 3.7085          |
| 3.7136        | 27.0  | 11637 | 0.0688   | 3.7103          |
| 3.7101        | 28.0  | 12068 | 0.0688   | 3.7064          |
| 3.7101        | 29.0  | 12499 | 0.0688   | 3.7073          |
| 3.7114        | 30.0  | 12930 | 0.0688   | 3.7077          |
| 3.712         | 31.0  | 13361 | 0.0569   | 3.7083          |
| 3.7108        | 32.0  | 13792 | 0.0688   | 3.7083          |
| 3.7123        | 33.0  | 14223 | 0.0688   | 3.7072          |
| 3.7084        | 34.0  | 14654 | 0.0688   | 3.7069          |
| 3.7112        | 35.0  | 15085 | 0.0688   | 3.7067          |
| 3.7098        | 36.0  | 15516 | 0.0688   | 3.7068          |
| 3.7098        | 37.0  | 15947 | 0.0688   | 3.7063          |
| 3.7098        | 38.0  | 16378 | 0.0688   | 3.7066          |
| 3.7097        | 39.0  | 16809 | 0.0688   | 3.7061          |
| 3.71          | 40.0  | 17240 | 0.0688   | 3.7073          |
| 3.71          | 41.0  | 17671 | 0.0688   | 3.7060          |
| 3.7081        | 42.0  | 18102 | 0.0688   | 3.7060          |
| 3.7091        | 43.0  | 18533 | 0.0688   | 3.7060          |
| 3.7091        | 44.0  | 18964 | 0.0688   | 3.7069          |
| 3.7097        | 45.0  | 19395 | 0.0688   | 3.7060          |
| 3.7113        | 46.0  | 19826 | 0.0688   | 3.7060          |
| 3.7078        | 47.0  | 20257 | 0.0688   | 3.7059          |
| 3.707         | 48.0  | 20688 | 0.0688   | 3.7058          |
| 3.7095        | 49.0  | 21119 | 0.0688   | 3.7058          |
| 3.7099        | 50.0  | 21550 | 0.0688   | 3.7057          |
| 3.7099        | 51.0  | 21981 | 3.7121   | 0.0688          |
| 3.7145        | 52.0  | 22412 | 3.7084   | 0.0688          |
| 3.7131        | 53.0  | 22843 | 3.7074   | 0.0688          |
| 3.7111        | 54.0  | 23274 | 3.7067   | 0.0688          |
| 3.7121        | 55.0  | 23705 | 3.7084   | 0.0688          |
| 3.7108        | 56.0  | 24136 | 3.7095   | 0.0688          |
| 3.7123        | 57.0  | 24567 | 3.7070   | 0.0688          |
| 3.7123        | 58.0  | 24998 | 3.7068   | 0.0688          |
| 3.7109        | 59.0  | 25429 | 3.7069   | 0.0688          |
| 3.7109        | 60.0  | 25860 | 3.7069   | 0.0688          |
| 3.7103        | 61.0  | 26291 | 3.7065   | 0.0688          |
| 3.713         | 62.0  | 26722 | 3.7074   | 0.0688          |
| 3.7106        | 63.0  | 27153 | 3.7066   | 0.0688          |
| 3.7098        | 64.0  | 27584 | 3.7065   | 0.0688          |
| 3.7112        | 65.0  | 28015 | 3.7070   | 0.0688          |
| 3.7112        | 66.0  | 28446 | 3.7098   | 0.0688          |
| 3.7114        | 67.0  | 28877 | 3.7062   | 0.0688          |
| 3.713         | 68.0  | 29308 | 3.7072   | 0.0688          |
| 3.7099        | 69.0  | 29739 | 3.7061   | 0.0688          |
| 3.709         | 70.0  | 30170 | 3.7078   | 0.0688          |
| 3.7116        | 71.0  | 30601 | 3.7074   | 0.0688          |
| 3.7104        | 72.0  | 31032 | 3.7070   | 0.0688          |
| 3.7104        | 73.0  | 31463 | 3.7069   | 0.0688          |
| 3.7088        | 74.0  | 31894 | 3.7062   | 0.0688          |
| 3.7108        | 75.0  | 32325 | 3.7064   | 0.0688          |
| 3.707         | 76.0  | 32756 | 3.7063   | 0.0688          |
| 3.7089        | 77.0  | 33187 | 3.7065   | 0.0688          |
| 3.7116        | 78.0  | 33618 | 3.7061   | 0.0688          |
| 3.7111        | 79.0  | 34049 | 3.7070   | 0.0688          |
| 3.7111        | 80.0  | 34480 | 3.7063   | 0.0688          |
| 3.7094        | 81.0  | 34911 | 3.7062   | 0.0688          |
| 3.7097        | 82.0  | 35342 | 3.7062   | 0.0688          |
| 3.7088        | 83.0  | 35773 | 3.7072   | 0.0688          |
| 3.7088        | 84.0  | 36204 | 3.7061   | 0.0688          |
| 3.7097        | 85.0  | 36635 | 3.7064   | 0.0688          |
| 3.7107        | 86.0  | 37066 | 3.7059   | 0.0688          |
| 3.7107        | 87.0  | 37497 | 3.7062   | 0.0688          |
| 3.7094        | 88.0  | 37928 | 3.7061   | 0.0688          |
| 3.7091        | 89.0  | 38359 | 3.7061   | 0.0688          |
| 3.7091        | 90.0  | 38790 | 3.7058   | 0.0688          |
| 3.7098        | 91.0  | 39221 | 3.7058   | 0.0688          |
| 3.7087        | 92.0  | 39652 | 3.7058   | 0.0688          |
| 3.7078        | 93.0  | 40083 | 3.7059   | 0.0688          |
| 3.7102        | 94.0  | 40514 | 3.7059   | 0.0688          |
| 3.7102        | 95.0  | 40945 | 3.7058   | 0.0688          |
| 3.7092        | 96.0  | 41376 | 3.7059   | 0.0688          |
| 3.7069        | 97.0  | 41807 | 3.7057   | 0.0688          |
| 3.7089        | 98.0  | 42238 | 3.7058   | 0.0688          |
| 3.7101        | 99.0  | 42669 | 3.7057   | 0.0688          |
| 3.7081        | 100.0 | 43100 | 3.7057   | 0.0688          |


### Framework versions

- Transformers 4.33.3
- Pytorch 2.1.2
- Datasets 2.16.1
- Tokenizers 0.13.3
