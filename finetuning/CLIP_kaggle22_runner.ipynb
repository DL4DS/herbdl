{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from typing import Generator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def convert_text_image_pairs_to_huggingface_json(root_csv, out_json):\n",
    "\n",
    "    if type(root_csv) is str:\n",
    "        df = pd.read_csv(root_csv)\n",
    "    else:\n",
    "        df = root_csv\n",
    "\n",
    "    root_path = '/mnt/purenfs/data/herbaria/train_images/'\n",
    "    with open(out_json, \"w\") as f:\n",
    "        written_count = 0\n",
    "        for index, row in df.iterrows():\n",
    "            line_dict = {\"image\": root_path+row['filename'], \"caption\":row['label']}\n",
    "            json_line = json.dumps(line_dict, indent=None, separators=(\",\",\":\"))\n",
    "            #print(json_line)\n",
    "            f.write(json_line + \"\\n\")\n",
    "            written_count += 1\n",
    "        print(f\"wrote {written_count} lines to {out_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the data folder of text/image pairs to a huggingface dataset-compatible json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace `root_folder` in the next cell with the top-level folder containing your images, and `out_json` with a path to where the json file representing the image/caption pairs in that folder should be saved.\n",
    "\n",
    "Note this only works with pairs of the form `filename.jpg`/`filename.txt` or `filename.jpeg`/`filename.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle 2022 Train and Val splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_csv = \"train_2022_labeled.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000__001</td>\n",
       "      <td>000/00/00000__001.jpg</td>\n",
       "      <td>Pinaceae Abies amabilis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000__002</td>\n",
       "      <td>000/00/00000__002.jpg</td>\n",
       "      <td>Pinaceae Abies amabilis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000__003</td>\n",
       "      <td>000/00/00000__003.jpg</td>\n",
       "      <td>Pinaceae Abies amabilis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000__004</td>\n",
       "      <td>000/00/00000__004.jpg</td>\n",
       "      <td>Pinaceae Abies amabilis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000__005</td>\n",
       "      <td>000/00/00000__005.jpg</td>\n",
       "      <td>Pinaceae Abies amabilis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_id               filename                    label\n",
       "0  00000__001  000/00/00000__001.jpg  Pinaceae Abies amabilis\n",
       "1  00000__002  000/00/00000__002.jpg  Pinaceae Abies amabilis\n",
       "2  00000__003  000/00/00000__003.jpg  Pinaceae Abies amabilis\n",
       "3  00000__004  000/00/00000__004.jpg  Pinaceae Abies amabilis\n",
       "4  00000__005  000/00/00000__005.jpg  Pinaceae Abies amabilis"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train22 = pd.read_csv(root_csv)\n",
    "\n",
    "def label_to_taxons(label):\n",
    "    label = label.split(\" \")\n",
    "    species, genus, family = label[6][:-1], label[10], label[13][:-1]\n",
    "\n",
    "    return species, genus, family\n",
    "\n",
    "train22['species'], train22['genus'], train22['family'] = zip(*train22['label'].map(label_to_taxons))\n",
    "train22['label'] = train22.apply(lambda x: x['family'] + ' ' + x['genus'] + ' ' + x['species'], axis=1)\n",
    "train22 = train22.drop(columns=['Unnamed: 0', 'species', 'genus', 'family'])\n",
    "\n",
    "train22.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((671817, 3), (167955, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val = train_test_split(train22, test_size=0.2, random_state=42)\n",
    "\n",
    "#train.to_csv(\"/projectnb/herbdl/data/kaggle-herbaria/train_2022_scientific.csv\", index=False)\n",
    "#val.to_csv(\"/projectnb/herbdl/data/kaggle-herbaria/val_2022_scientific.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "train.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 671817 lines to train_22_encoded.json\n",
      "wrote 167955 lines to val_22_encoded.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((671817, 3), (167955, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encode the label\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train['label'])\n",
    "train['label'] = label_encoder.transform(train['label'])\n",
    "val['label'] = label_encoder.transform(val['label'])\n",
    "\n",
    "convert_text_image_pairs_to_huggingface_json(train, \"train_22_encoded.json\")\n",
    "convert_text_image_pairs_to_huggingface_json(val, \"val_22_encoded.json\")\n",
    "\n",
    "train.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take small subset of train and val and save it\n",
    "train_subset = train.sample(n=1000, random_state=42)\n",
    "val_subset = val.sample(n=200, random_state=42)\n",
    "\n",
    "train_subset.to_csv(\"/projectnb/herbdl/data/kaggle-herbaria/train_2022_labeled2.csv\", index=False)\n",
    "val_subset.to_csv(\"/projectnb/herbdl/data/kaggle-herbaria/val_2022_labeled2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>686446</th>\n",
       "      <td>12640__012</td>\n",
       "      <td>126/40/12640__012.jpg</td>\n",
       "      <td>Roldana cordovensis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83405</th>\n",
       "      <td>01612__004</td>\n",
       "      <td>016/12/01612__004.jpg</td>\n",
       "      <td>Astragalus sophoroides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155613</th>\n",
       "      <td>02962__081</td>\n",
       "      <td>029/62/02962__081.jpg</td>\n",
       "      <td>Carex whitneyi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455876</th>\n",
       "      <td>08401__098</td>\n",
       "      <td>084/01/08401__098.jpg</td>\n",
       "      <td>Lilium philadelphicum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141438</th>\n",
       "      <td>02707__054</td>\n",
       "      <td>027/07/02707__054.jpg</td>\n",
       "      <td>Carex hoodii</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id               filename                   label\n",
       "686446  12640__012  126/40/12640__012.jpg     Roldana cordovensis\n",
       "83405   01612__004  016/12/01612__004.jpg  Astragalus sophoroides\n",
       "155613  02962__081  029/62/02962__081.jpg          Carex whitneyi\n",
       "455876  08401__098  084/01/08401__098.jpg   Lilium philadelphicum\n",
       "141438  02707__054  027/07/02707__054.jpg            Carex hoodii"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([167955, 14])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_CPKT = \"openai/clip-vit-large-patch14-336\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CPKT, cache_dir=\"../kaggle_eval/\")\n",
    "\n",
    "labels_tokenized = tokenizer(val['label'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "labels_tokenized['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 671817 lines to /projectnb/herbdl/workspaces/faridkar/herbdl/finetuning/train.json\n",
      "wrote 167955 lines to /projectnb/herbdl/workspaces/faridkar/herbdl/finetuning/val.json\n"
     ]
    }
   ],
   "source": [
    "train_json = \"/projectnb/herbdl/workspaces/faridkar/herbdl/finetuning/train.json\"\n",
    "val_json = \"/projectnb/herbdl/workspaces/faridkar/herbdl/finetuning/val.json\"\n",
    "\n",
    "convert_text_image_pairs_to_huggingface_json(\"/projectnb/herbdl/data/kaggle-herbaria/train_2022_labeled2.csv\", train_json)\n",
    "convert_text_image_pairs_to_huggingface_json(\"/projectnb/herbdl/data/kaggle-herbaria/val_2022_labeled2.csv\", val_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that it worked by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(671817, 167955)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test loading it back in\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"json\", data_files=train_json)\n",
    "val_dataset = load_dataset(\"json\", data_files=val_json)\n",
    "\n",
    "len(train_dataset['train']), len(val_dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the finetuning\n",
    "\n",
    "### Configuration\n",
    "\n",
    "`repo_id` - The starting point for finetuning. By default this uses the `openai/clip-vit-large-patch14-336` pre-trained CLIP weights. This is what Stable Diffusion versions up to 1.5 used. Another option you might want to consider is `laion/CLIP-ViT-H-14-laion2B-s32B-b79K`, which was used for Stable Diffusion 2.0 onwards.\n",
    "\n",
    "`output_folder` - Where to store the output. The saving process writes multiple files to this folder, so it should be empty.\n",
    "\n",
    "`batch_size` - Training batch size. Don't go lower than 8 - try 32 or 64 (unless you only have a few images).\n",
    "\n",
    "`num_train_epochs` - How many epochs to train. With <500 images each epoch on a 3090 takes a few minutes - do a small number, say `3` to start with, and check the loss when it's done before increasing the number of epochs. With 3 epochs my loss went down to around 2. After 10 epochs it was down to 0.63. Be careful not to over-fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id =  \"openai/clip-vit-large-patch14-336\"\n",
    "output_folder = \"/projectnb/herbdl/workspaces/faridkar/finetuning/output/finetuned-kaggle-2022-05-06\"\n",
    "batch_size = 8\n",
    "num_train_epochs = 1\n",
    "\n",
    "train_json = \"/projectnb/herbdl/workspaces/faridkar/herbdl/finetuning/train.json\"\n",
    "val_json = \"/projectnb/herbdl/workspaces/faridkar/herbdl/finetuning/val.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning openai/clip-vit-large-patch14-336 for 1 epochs with batch size 8, and then saving output to /projectnb/herbdl/workspaces/faridkar/finetuning/output/finetuned-kaggle-2022-05-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/07/2024 06:49:54 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2distributed training: True, 16-bits training: False\n",
      "/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/datasets/load.py:2483: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Generating train split: 671817 examples [00:00, 1802476.77 examples/s]\n",
      "Generating validation split: 167955 examples [00:00, 1825484.14 examples/s]\n",
      "/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "[WARNING|configuration_clip.py:337] 2024-08-07 06:49:54,914 >> `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "[WARNING|configuration_clip.py:337] 2024-08-07 06:49:54,914 >> `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "[WARNING|configuration_clip.py:337] 2024-08-07 06:49:54,914 >> `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "Filter: 100%|██████████████████| 671817/671817 [03:53<00:00, 2880.56 examples/s]\n",
      "Running tokenizer on train dataset: 100%|█| 671817/671817 [00:12<00:00, 55596.95\n",
      "Parameter 'transform'=<function main.<locals>.transform_images at 0x14b7d0a9aca0> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "08/07/2024 06:54:02 - WARNING - datasets.fingerprint - Parameter 'transform'=<function main.<locals>.transform_images at 0x14b7d0a9aca0> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Filter: 100%|██████████████████| 167955/167955 [00:58<00:00, 2868.23 examples/s]\n",
      "Running tokenizer on validation dataset: 100%|█| 167955/167955 [00:02<00:00, 564\n",
      "/usr3/graduate/faridkar/.local/lib/python3.11/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "08/07/2024 06:55:04 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "  0%|                                                 | 0/41989 [00:00<?, ?it/s]/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.9976, 'learning_rate': 4.940460596822978e-05, 'epoch': 0.01}         \n",
      "  1%|▍                                    | 500/41989 [06:39<9:21:48,  1.23it/s]/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.8225, 'learning_rate': 4.880921193645955e-05, 'epoch': 0.02}         \n",
      "  2%|▊                                   | 1000/41989 [13:27<8:58:16,  1.27it/s]/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.6873, 'learning_rate': 4.8213817904689324e-05, 'epoch': 0.04}        \n",
      "  4%|█▎                                  | 1500/41989 [20:10<8:57:23,  1.26it/s]/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.5664, 'learning_rate': 4.76184238729191e-05, 'epoch': 0.05}          \n",
      "  5%|█▋                                  | 2000/41989 [26:53<8:46:28,  1.27it/s]/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.4547, 'learning_rate': 4.702302984114887e-05, 'epoch': 0.06}         \n",
      "  6%|██▏                                 | 2500/41989 [33:37<8:41:22,  1.26it/s]/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  7%|██▌                                 | 2956/41989 [39:45<8:33:35,  1.27it/s]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/projectnb/herbdl/workspaces/faridkar/herbdl/finetuning/CLIP_finetuning.py\", line 537, in <module>\n",
      "    main()\n",
      "  File \"/projectnb/herbdl/workspaces/faridkar/herbdl/finetuning/CLIP_finetuning.py\", line 508, in main\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/transformers/trainer.py\", line 1556, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/transformers/trainer.py\", line 1838, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/transformers/trainer.py\", line 2704, in training_step\n",
      "    self.accelerator.backward(loss)\n",
      "  File \"/usr3/graduate/faridkar/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 2127, in backward\n",
      "    loss.backward(**kwargs)\n",
      "  File \"/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "  7%|██▌                                 | 2956/41989 [39:46<8:45:13,  1.24it/s]\n",
      "--\n",
      "DONE\n",
      "If it worked, trained data should be in /projectnb/herbdl/workspaces/faridkar/finetuning/output/finetuned-kaggle-2022-05-06\n"
     ]
    }
   ],
   "source": [
    "print(f\"Finetuning {repo_id} for {num_train_epochs} epochs with batch size {batch_size}, and then saving output to {output_folder}.\")\n",
    "!python CLIP_finetuning.py \\\n",
    "    --output_dir {output_folder} \\\n",
    "    --model_name_or_path {repo_id} \\\n",
    "    --train_file {train_json} \\\n",
    "    --validation_file {val_json} \\\n",
    "    --image_column image \\\n",
    "    --overwrite_output_dir=True \\\n",
    "    --max_seq_length=35 \\\n",
    "    --num_train_epochs={num_train_epochs} \\\n",
    "    --caption_column caption \\\n",
    "    --remove_unused_columns=False \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --per_device_train_batch_size={batch_size} \\\n",
    "    --learning_rate=\"5e-5\" --warmup_steps=\"0\" --weight_decay 0.1 \n",
    "print(\"--\\nDONE\")\n",
    "print(f\"If it worked, trained data should be in {output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it all worked, your finetuned CLIP model is in the `output_folder` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
